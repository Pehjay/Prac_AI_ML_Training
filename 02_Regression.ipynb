{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXHHGaTBAkiv"
   },
   "source": [
    "# Hands-on Session 2: Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYx1rduyFEKd"
   },
   "source": [
    "## House Price Prediction\n",
    "In this hands on session, we will learn how to perform prediction on house prices given a set of attributes for houses on sale. The dataset contains 13 attributes/features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXy0y3KGFJE0"
   },
   "source": [
    "1\t**Id**: To count the records.\n",
    "\n",
    "2\t**MSSubClass**: Identifies the type of dwelling involved in the sale.\n",
    "\n",
    "3\t**MSZoning**: Identifies the general zoning classification of the sale.\n",
    "\n",
    "4\t**LotArea**: Lot size in square feet.\n",
    "\n",
    "5\t**LotConfig**:\tConfiguration of the lot\n",
    "\n",
    "6\t**BldgType**:\tType of dwelling\n",
    "\n",
    "7\t**OverallCond**:\tRates the overall condition of the house\n",
    "\n",
    "8\t**YearBuilt**:\tOriginal construction year\n",
    "\n",
    "9\t**YearRemodAdd**:\tRemodel date (same as construction date if no remodeling or additions).\n",
    "\n",
    "10\t**Exterior1st**:\tExterior covering on house\n",
    "\n",
    "11\t**BsmtFinSF2**: Type 2 finished square feet.\n",
    "\n",
    "12\t**TotalBsmtSF**: Total square feet of basement area\n",
    "\n",
    "13\t**SalePrice**: To be predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9eFPBCcFTHD"
   },
   "source": [
    "## Importing Libraries and Dataset\n",
    "First, we will import the following libraries that will be used specific purpose as stated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIIW7cQQAc4e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "dataset = pd.read_excel(\"HousePricePrediction.xlsx\")\n",
    "\n",
    "# Printing first 5 records of the dataset\n",
    "print(dataset.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mou2skxDBKtE"
   },
   "source": [
    "We can use the shape function to show us the dimension of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mu_83aUZBLOy"
   },
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describing the numerical and categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.select_dtypes('object').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8VFNF9IBOiy"
   },
   "source": [
    "## Data Preprocessing\n",
    "Now, we categorize the features depending on their datatype (int, float, object) and then compute the number of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjlGmxUeBQzA"
   },
   "outputs": [],
   "source": [
    "obj = (dataset.dtypes == 'object')\n",
    "object_cols = list(obj[obj].index)\n",
    "print(\"Categorical variables:\",len(object_cols))\n",
    "\n",
    "int_ = (dataset.dtypes == 'int')\n",
    "num_cols = list(int_[int_].index)\n",
    "print(\"Integer variables:\",len(num_cols))\n",
    "\n",
    "fl = (dataset.dtypes == 'float')\n",
    "fl_cols = list(fl[fl].index)\n",
    "print(\"Float variables:\",len(fl_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnjBfAjSBYqK"
   },
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "EDA refers to the deep analysis of data that allows us to discover different patterns and spot anomalies. Before making inferences from data it is essential to examine and explore the dataset.\n",
    "\n",
    "First, we will visualise a **heatmap** using seaborn library. The heatmap allows us to the the correlation between the attributes in the dataset and allow use to understand our dataset easier in a visual manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWmkU2yFBj-L"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(dataset.corr(numeric_only=True),\n",
    "            cmap = 'BrBG',\n",
    "            fmt = '.2f',\n",
    "            linewidths = 2,\n",
    "            annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbJkd3dMBnbm"
   },
   "source": [
    "Next, we draw a **barplot** to analyze the different categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwZ63EQRBpfW"
   },
   "outputs": [],
   "source": [
    "unique_values = []\n",
    "for col in object_cols:\n",
    "  unique_values.append(dataset[col].unique().size)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('No. Unique values of Categorical Features')\n",
    "plt.xticks(rotation=90)\n",
    "sns.barplot(x=object_cols,y=unique_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3K9CwW0UBtsO"
   },
   "source": [
    "The plot shows that *Exterior1st* has around 16 unique categories and other features have around  6 unique categories. To findout the actual count of each category we can plot the bargraph of each four features separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUrzYiySBvAo"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.title('Categorical Features: Distribution')\n",
    "plt.xticks([],rotation=90),plt.yticks([])\n",
    "index = 1\n",
    "\n",
    "for col in object_cols:\n",
    "    y = dataset[col].value_counts()\n",
    "    plt.subplot(1, 4, index)\n",
    "    plt.xticks(rotation=90)\n",
    "    sns.barplot(x=list(y.index), y=y)\n",
    "    index += 1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXMcnL8LBzDn"
   },
   "source": [
    "## Data Cleaning\n",
    "Data Cleaning is a very important step to improvise the data or remove incorrect, corrupted or irrelevant data before we performed prediction with the dataset.\n",
    "\n",
    "Unlike the Iris dataset we explored in Hands-on Session 1, not all attributes are important and relevant for the model training. So, we can drop these non-useful attributes/column before training. In addition, it is quite common that the dataset contains missing values. There are 2 approaches to dealing with empty/null values:\n",
    "\n",
    "1) We can delete the column/row (if the feature or record is not much important).\n",
    "\n",
    "2) Filling the empty slots with mean/mode/0/NA/etc. (depending on the dataset requirement).\n",
    "\n",
    "First, we can drop the *Id* Column that will not be used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqeS5jQ2B4yO"
   },
   "outputs": [],
   "source": [
    "dataset.drop(['Id'],\n",
    "             axis=1,\n",
    "             inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZZW1uIpB-f5"
   },
   "source": [
    "Next, we replace SalePrice empty values with their mean values to make the data distribution symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQu9jSf5Bxjx"
   },
   "outputs": [],
   "source": [
    "dataset['SalePrice'] = dataset['SalePrice'].fillna(\n",
    "  dataset['SalePrice'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIIzbxE1CBrZ"
   },
   "source": [
    "Since there are very few empty records, we drop records with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcttyLhUCCib"
   },
   "outputs": [],
   "source": [
    "new_dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUjHCq2aCHHE"
   },
   "source": [
    "After performing the data cleaning, we now check features which have null values in the new dataframe (if there are still any).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9CYOkKfCJF5"
   },
   "outputs": [],
   "source": [
    "new_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9-tJu2hCL_B"
   },
   "source": [
    "## OneHotEncoder â€“ For Label categorical features\n",
    "One hot Encoding is a way to convert categorical data into binary vectors. This maps the values to integer values. By using OneHotEncoder, object data will be converted into int.\n",
    "\n",
    "Before, we apply OneHotEncoding, we first find all the features which have the object datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-158QGr1CQIi"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "s = (new_dataset.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)\n",
    "print('No. of. categorical features: ',\n",
    "      len(object_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb5N1Y45CTXW"
   },
   "source": [
    "Then, we can apply OneHotEncoding to the whole list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIw6JZV2CTyV"
   },
   "outputs": [],
   "source": [
    "OH_encoder = OneHotEncoder(sparse_output=False)\n",
    "OH_cols = pd.DataFrame(OH_encoder.fit_transform(new_dataset[object_cols]))\n",
    "OH_cols.index = new_dataset.index\n",
    "OH_cols.columns = OH_encoder.get_feature_names_out()\n",
    "# df_final = new_dataset.drop(object_cols, axis=1)\n",
    "# df_final = pd.concat([df_final, OH_cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzcQCwnJX9zL"
   },
   "outputs": [],
   "source": [
    "OH_cols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization - For Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler #z-score normalization\n",
    "df_year = new_dataset[['YearBuilt','YearRemodAdd']]\n",
    "df_numerical = new_dataset.drop(object_cols, axis=1).drop(['YearBuilt','YearRemodAdd','SalePrice'],axis=1).copy()\n",
    "scaler = StandardScaler()\n",
    "df_num_norm = pd.DataFrame(scaler.fit_transform(df_numerical),columns=df_numerical.columns)\n",
    "df_num_norm.index = new_dataset.index\n",
    "df_final = pd.concat([df_num_norm,df_year, OH_cols], axis=1)\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45sZ9_nhCV4W"
   },
   "source": [
    "## Splitting Dataset into Training and Testing\n",
    "X and Y splitting (i.e. Y is the SalePrice column and the rest of the other columns are X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LPjkHTUCYgH"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_final#.drop(['SalePrice'], axis=1)\n",
    "Y = new_dataset['SalePrice']\n",
    "\n",
    "# Split the training set into\n",
    "# training and validation set\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(\n",
    "    X, Y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhsVEKAwCbnL"
   },
   "source": [
    "## Model Training and Evaluation\n",
    "As house price is a  continuous value, we will use regression models to train the house prediction model. We will train the predictio model with 3 popular machine learning algorithms and compare their results:\n",
    "\n",
    "1) SVM-Support Vector Machine\n",
    "2) Random Forest Regressor\n",
    "3) Linear Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySIRDKL7CldF"
   },
   "source": [
    "And To calculate loss we will be using the **mean_absolute_percentage_error** module. It can easily be imported by using sklearn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PYlk4vQC6jU"
   },
   "source": [
    "## SVM â€“ Support Vector Machine\n",
    "SVM can be used for both regression and classification model. It finds the hyperplane in the n-dimensional plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LU7i1gjQCeMv"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "model_SVR = svm.SVR(kernel=\"linear\", C=10, epsilon = 0.1, gamma = 0.1)\n",
    "model_SVR.fit(X_train,Y_train)\n",
    "Y_pred = model_SVR.predict(X_valid)\n",
    "\n",
    "#Calculate MAE\n",
    "mae = mean_absolute_error(Y_valid, Y_pred)\n",
    "range_y = max(Y_train) - min(Y_train)\n",
    "nmae = mae / range_y\n",
    "print(f\"MAE: {nmae}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(Y_valid, Y_pred)\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "X_pca = pca.fit_transform(X_valid)\n",
    "# Plot the true values\n",
    "plt.scatter(X_pca, Y_valid, color='blue', label='True values')\n",
    "\n",
    "# Plot the predicted values\n",
    "plt.scatter(X_pca, Y_pred, color='red', label='Predicted values')\n",
    "# Add title and labels\n",
    "plt.title('SVM Regression: Predictions vs Real Values')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target Value')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tflqi-pjDX6W"
   },
   "source": [
    "## Random Forest Regression\n",
    "Random Forest is an ensemble technique that uses multiple of decision trees and can be used for both regression and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5C-xi9FeDb9O"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_RFR = RandomForestRegressor(n_estimators=100)\n",
    "model_RFR.fit(X_train, Y_train)\n",
    "Y_pred = model_RFR.predict(X_valid)\n",
    "\n",
    "#Calculate MAE\n",
    "mae = mean_absolute_error(Y_valid, Y_pred)\n",
    "range_y = max(Y_train) - min(Y_train)\n",
    "nmae = mae / range_y\n",
    "print(f\"MAE: {nmae}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(Y_valid, Y_pred)\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "X_pca = pca.fit_transform(X_valid)\n",
    "# Plot the true values\n",
    "plt.scatter(X_pca, Y_valid, color='blue', label='True values')\n",
    "\n",
    "# Plot the predicted values\n",
    "plt.scatter(X_pca, Y_pred, color='red', label='Predicted values')\n",
    "# Add title and labels\n",
    "plt.title('RF Regression: Predictions vs Real Values')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target Value')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM8sO-WRDaf0"
   },
   "source": [
    "## Linear Regression\n",
    "Linear Regression predicts the final output-dependent value based on the given independent features. For example, here we have to predict SalePrice depending on features like MSSubClass, YearBuilt, BldgType, Exterior1st etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzDA_lgnDf_s"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_LR = LinearRegression()\n",
    "model_LR.fit(X_train, Y_train)\n",
    "Y_pred = model_LR.predict(X_valid)\n",
    "\n",
    "#Calculate MAE\n",
    "mae = mean_absolute_error(Y_valid, Y_pred)\n",
    "range_y = max(Y_train) - min(Y_train)\n",
    "nmae = mae / range_y\n",
    "print(f\"MAE: {nmae}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(Y_valid, Y_pred)\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "X_pca = pca.fit_transform(X_valid)\n",
    "# Plot the true values\n",
    "plt.scatter(X_pca, Y_valid, color='blue', label='True values')\n",
    "\n",
    "# Plot the predicted values\n",
    "plt.scatter(X_pca, Y_pred, color='red', label='Predicted values')\n",
    "# Add title and labels\n",
    "plt.title('Linear Regression: Predictions vs Real Values')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target Value')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model, encoder, scaler, and more...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model_LR, open(\"regressor.pkl\", \"wb\"))\n",
    "pickle.dump(OH_encoder, open(\"onehot.pkl\",\"wb\"))\n",
    "pickle.dump(scaler, open(\"regress_norm.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_values = {}\n",
    "max_values = {}\n",
    "categories = {}\n",
    "\n",
    "# Loop through the DataFrame columns\n",
    "for col in dataset.columns:\n",
    "    if dataset[col].dtype in ['int64', 'float64']:  # Numerical columns\n",
    "        min_values[col] = dataset[col].min()\n",
    "        max_values[col] = dataset[col].max()\n",
    "    elif dataset[col].dtype.name == 'object':  # Categorical columns\n",
    "        categories[col] = dataset[col].astype('category').cat.categories.tolist()\n",
    "\n",
    "# Print the results\n",
    "print(\"Min values:\", min_values)\n",
    "print(\"Max values:\", max_values)\n",
    "print(\"Categories:\", categories)\n",
    "\n",
    "# Combine the min, max, and categories into a dictionary\n",
    "min_max_data = {\n",
    "    'Min values': min_values,\n",
    "    'Max values': max_values,\n",
    "    'Categories': categories\n",
    "}\n",
    "\n",
    "# Save to a Pickle file\n",
    "with open('min_max_and_categories.pkl', 'wb') as f:\n",
    "    pickle.dump(min_max_data, f)\n",
    "\n",
    "print(\"Data saved to min_max_and_categories.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iVVyQMUHbD6"
   },
   "source": [
    "## Take Home Exercise\n",
    "Work on the Life Satisfaction data and GDP per capita data which has been provided for you. The CSV files were obtained publicly from:\n",
    "* (http://stats.oecd.org/index.aspx?DataSetCode=BLI) (Life Satisfaction data from OECD)\n",
    "* (http://goo.gl/j1MSKe) (GDP per capita data from IMF)\n",
    "\n",
    "**Goal**: We want to find out if money makes people happy. To take it further: If we knew the GDP per capita of a certain country, can we predict its level of life satisfaction?\n",
    "\n",
    "Try the following:\n",
    "* Join the data tables and sort by GDP per capita.\n",
    "* Plot the data for selected countries to visualize its distribution. Find out if there's any trend.\n",
    "* Model \"Life satisfaction\" as a function of \"GDP per capita\".\n",
    "* Find the parameters of the model through training.\n",
    "* Test out the model by evaluating it with some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
